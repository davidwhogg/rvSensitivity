% This document is part of the rvSensitivity project.
% Copyright 2020 the authors. All rights reserved.

% to-do
% -----
% - draft
% - send to friendlies
% - post to arXiv
% - profit

% style notes
% -----------
% - use \, for multiplication
% - put punctuation after equations as necessary, with "\hquad,". Use \hquad
% - put term name definitions in \textsl{}, *not* quotations
% - don't say "i.e.," say "that is,". Don't say "e.g.," say "for example,".
% - ...[your pet peeve here]...

\documentclass[10pt, letterpaper]{article}

\usepackage{amsmath, bm, mathrsfs, amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks,
            colorlinks=true,
            linkcolor=NavyBlue,
            citecolor=darkgray,
            urlcolor=NavyBlue]{hyperref}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{doi}

\usepackage{natbib}
\bibliographystyle{hogg_abbrvnat}
\setcitestyle{round,citesep={,},aysep={}}

% text stuhh
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\equationname}{equation}
\newcommand{\notename}{Note}
\renewcommand{\tablename}{Table}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et~al.}}

% math stuhh
\newcommand{\hquad}{~~}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}\!}}
\newcommand{\inv}{^{-1}}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\mean}[1]{\left<{#1}\right>}
\newcommand{\expectation}[1]{\mathbb{E}\!\left[{#1}\right]}
\newcommand{\scalar}[1]{#1}
\renewcommand{\vector}[1]{\boldsymbol{#1}}
\newcommand{\vu}{\vector{\hat{u}}}
\newcommand{\vv}{\vector{v}}
\newcommand{\valpha}{\vector{\alpha}}
\newcommand{\vtheta}{\vector{\theta}}
\newcommand{\tensor}[1]{\mathbf{#1}}
\newcommand{\tC}{\tensor{C}}
\renewcommand{\matrix}[1]{\mathsf{#1}}
\newcommand{\mA}{\matrix{A}}
\newcommand{\normal}{\mathcal{N}\!\,}
\newcommand{\uniform}{\mathcal{U}\!\,}
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\BJD}{\unit{BJD}}
\newcommand{\kms}{\unit{km}\,\unit{s}^{-1}}

% Journal keys - kill me now
\newcommand{\aj}{Astron.\,J.}
\newcommand{\apj}{Astrophys.\,J.}
\newcommand{\apjs}{Astrophys.\,J.\,Supp.\,Ser.}

% page layout stuff
\setlength{\topmargin}{0.0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\parindent}{\baselineskip}
\setlength{\oddsidemargin}{-0.2in}
\setlength{\textwidth}{4.3in}
\setlength{\textheight}{2\textwidth}
\raggedbottom\sloppy\sloppypar\frenchspacing

% this might be crazy, but here's my number
\setlength{\marginparsep}{0.15in}
\setlength{\marginparwidth}{2.7in}
\usepackage{marginfix} % necessary but possibly evil
\newcounter{marginnote}
\setcounter{marginnote}{0}
\renewcommand{\footnote}[1]{\refstepcounter{marginnote}\textsuperscript{\themarginnote}\marginpar{\color{darkgray}\raggedright\footnotesize\textsuperscript{\themarginnote}#1}}
\newcommand{\tfigurerule}{\rule{0pt}{1ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{0.25ex}}
\newcommand{\bfigurerule}{\rule{0pt}{0.25ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{1ex}}
\renewcommand{\caption}[1]{\parbox{\marginparwidth}{\footnotesize\textbf{\refstepcounter{figure}\figurename~\thefigure}: {#1}}}

% kill these at submission
\newcommand{\bedell}[1]{\textcolor{red}{[Bedell says: #1]}}
\newcommand{\hogg}[1]{\textcolor{magenta}{[Hogg says: #1]}}

\begin{document}

\section*{What is the sensitivity of a \\
  stellar radial-velocity observing program \\
  to an orbiting planet?}

\noindent\textbf{David W. Hogg}\footnote{%
  The authors would like to thank
  [put names here]
  for help with all these concepts.
  This research was supported by the National Science Foundation and National Aeronautics and Space Administration.}\\
{\footnotesize%
  \textsl{Center for Cosmology and Particle Physics, Dept.\ Physics, New York University}\\
  \textsl{Max-Planck-Institut f\"ur Astronomie, Heidelberg}\\
  \textsl{Flatiron Institute, a division of the Simons Foundation}%
}

\medskip\noindent\textbf{Megan Bedell}\\
{\footnotesize%
  \textsl{Flatiron Institute, a division of the Simons Foundation}%
}

\paragraph{Abstract:}
We find planets and binary stars by (among other techniques) the radial-velocity
method.
Here we use information theory to estimate---or really bound---the sensitivity
of a planned or actual radial-velocity survey.
We find interesting things.

\section{Introduction}

Hello World!\footnote{Hopefully we will find a reason to cite
  \cite{wobble} but this placeholder is just so that the bibliography
  ``does something''.}
Stuff and things.

The astronomical community has found a boatload of exoplanets in the
last decade or so\footnote{WHAT TO CITE?}.
A lot of these have been either found, or confirmed, or characterized,
using radial-velocity measurements.
The orbiting planet modulates the radial velocity of the star because
both the planet and the star orbit a common center of mass\footnote{CITE SOME REVIEW?}.

In the exoplanet community, there are a few conventions that we want
to highlight or challenge.
The first is that exoplanet science is often broken into a first stage
of \textsl{search} (or \textsl{discovery}), and a second stage of
\textsl{characterization}.
In fact these two stages are extremely similar, in the sense that an
exoplanet is discovered, in some sense, by the point that it can be
characterized.
For example, if it is decided that a planet detection is viable if it is,
say, ten-sigma, that is equivalent to saying that you know the amplitude
of the radial-velocity sigma with an uncertainty of ten-percent or better.
So in this \documentname\ we will not distinguish between search and
characterization.
Or, more correctly, we will treat the question of sensitivity of a
project to a planet is a question about how well that planet could be
characterized.

The second convention we want to highlight is that most questions that
have been asked about sensitivities of planet searches to planets have
been \foreign{ex post facto} and empirical.
They involve things like empirical false alarm
probabilities\footnote{MB WHAT TO CITE?}  estimated by randomizing the
data in various ways.
This is all laudable and important.
But sometimes we want to understand what an exoplanet project or survey
or program could or should detect. We want to forecast. And we want to use
forecasts to make good design decisions. We want to plan to use future
expensive observing resources sensibly and efficiently.
Here we lay out what that kind of forecasting might look like.

Don't read us wrong, however: In general, empirical estimates of
uncertainties, completeness, contamination, and sensitivity are safer
than theoretical estimates!
Forecasting---coming as it does \emph{before} any data---is necessarily
a theoretical activity.
As such, it requires that we make strong assumptions about what our data
\emph{will be like}.
We will have to make many assumptions, and most of them will turn out to
be wrong in detail.
Unavoidably, forecasts are summaries of our assumptions (about hardware,
observing conditions, and our targets).

As we will discuss below, our dependence on our assumptions is a bit weaker
if we treat forecasts as \emph{bounds}. When a forecast is seen not as telling
us how an experiment \emph{will do} but rather what an experiment \emph{could do}
in the best of all possible worlds, the forecast is slightly less conditional
on our strongest assumptions. But of course at the  expense of a lot of
its specificity.

We wrote this \documentname\ to help us think about how we might help
the \textsl{Terra Hunting Experiment}\footnote{CITE} plan a big,
ambitious, long-term project to discover and characterize Earth-like
planets around Sun-like stars on year-ish-period orbits. But of course the
methods presented below are applicable to any new project, and can also be
used to compare empirical \foreign{ex post facto} sensitivity estimates to
theoretical bounds for current and past projects.

\section{The problem and solution}

As we note above, we consider the question of planet detection to
reduce to the question of sensitivity of a survey to (say) the
velocity amplitude of the detected planet. For this reason, we can solve
our forecasting problems by solving a single forecasting problem, which we
state now.

\paragraph{Problem:}
Imagine that at $N$ barycentric times $t_n$ (given in $\BJD$, say) you have
measured the barycentric radial velocity $v$ of a star with uncertainty $\sigma_n$
(given in $\kms$, say). What is the sensitivity of this set of
observations to a single orbital companion with radial-velocity amplitude $\kappa$
orbiting at period $P$ and eccentricity $e$? Assume that the observer is located
isotropically relative to targets or \foreign{vice versa}, and that we
are not observing at any special time.

\paragraph{Solution:}
A single orbital companion can be parameterized by a velocity
amplitude $\kappa$ (velocity units; proportional to $M\,\sin i$), a
period $P$ (time units), an eccentricity $e$ (dimensionless), an
argument of perihelion angle $\varpi$ (in radians), and a phase $t_0$
(time). The last of these parameters could be the time at which the
companion is at conjunction or it could be the time at pericenter, or
it could be thought of as the zero of time. For any setting of these
parameters, there is a prediction $v(t_n)$ for the barycentric radial velocity of
the star at any time $t_n$ that looks like:
\begin{equation}
  v(t_n) = \kappa\,\xi(t_n - t_0; P, e, \varpi) + v_0
  \hquad,
\end{equation}
where $\xi(\Delta t; P, e, \varpi)$ is a dimensionless radial-velocity
prediction for the Kepler problem\footnote{We need to cite into some classics,
  and maybe also our own packages by DFM and APW. Note that there is some subtlety
  to the definition of $\xi()$ in amplitude: It really should have units of
  $\dd v/\dd\kappa$, and the definition of $\xi()$ sets the definition or
  interpretation of $\kappa$.)}
and $v_0$ is the relative radial velocity between this binary's barycenter and
the Solar System barycenter.
Note that the parameters $\kappa, v_0$ enter the expectation linearly, and
the parameters $t_0, P, e, \varpi$ enter nonlinearly. This distinction is very
important for what follows\footnote{And we have written overly extensively on
  this elsewhere CITE LUGER, HOGG.}.

Although we don't know what priors to put on $P$ or $e$ or $\kappa$---and
we don't need to put priors on any of these---we are told that we are to
treat the angular and time distributions as isotropic.
For this reason, we can assume that we do have prior pdfs over
the argument of perihelion $\varpi$ and the reference time $t_0$:
\begin{align}
  p(t_0\given P) &= \uniform(t_0\given t_{\min}, t_{\min}+P)
  \\
  p(\varpi) &= \uniform(\varpi\given 0, 2\pi)
  \hquad ,
\end{align}
where $\uniform(x\given a,b)$ is the uniform pdf for $x$ in the range
$a<x<b$, and $t_{\min}$ is some chosen reference time (like, for
example, the first time $t_1$ in the time series, but it could be
anything\footnote{Note that we don't lose any generality by choosing
  this $t_{\min}$: The function is cyclic in $t_0$ with period $P$. So
  a uniform pdf in any time interval of length $P$ would be equivalent
  here.}).
The prior $p(t_0\given P)$ on $t_0$ is conditional on the period $P$
because the period $P$ appears in its explicit form.

With these parts, we will build two matrices. The first is the $N\times N$
\textsl{inverse covariance matrix} or sometimes \textsl{precision matrix} $\tC_v\inv$
\begin{equation}
  \tC_v\inv = \left[\begin{array}{c c c c c}
      1/\sigma_1^2 & 0 & 0 & \cdots & 0 \\
      0 & 1/\sigma_2^2 & 0 & \cdots & 0 \\
      0 & 0 & 1/\sigma_3^2 & \cdots & 0 \\
      \vdots & \vdots & \vdots &    & \vdots \\
      0 & 0 & 0 & \cdots & 1/\sigma_N^2
    \end{array}\right]
\hquad ,
\end{equation}
and the second is the $N\times 2$ \textsl{design matrix} $\mA$
\begin{equation}
  \mA(\valpha) = \left[\begin{array}{c c}
      \xi(t_1 - t_0; P, e, \varpi) & 1 \\
      \xi(t_2 - t_0; P, e, \varpi) & 1 \\
      \xi(t_3 - t_0; P, e, \varpi) & 1 \\
      \vdots & \vdots \\
      \xi(t_N - t_0; P, e, \varpi) & 1
    \end{array}\right]
\hquad ,
\end{equation}
where the design matrix\footnote{DESCRIBE SYNTACTIC TYPOGRAPHY HERE.}
$\mA$ is a function of not just the times $\setof{t_n}_{n=1}^N$
but also the nonlinear parameters $\valpha\T\equiv[t_0, P, e, \varpi]$.
Note that the design matrix is the derivative of the velocity $v$
expectations with respect to the linear parameters $\vtheta\T\equiv [\kappa, v_0]$,
where the velocity expectations have been assembled, effectively,
into a column vector $\vv$ and the linear parameters into a column vector $\vtheta$.
That is,
\begin{equation}
  \mA(\valpha) = \left[\frac{\dd \vv}{\dd\vtheta}\right]_{\valpha}
\hquad .
\end{equation}

Now the best possible $2\times 2$ precision matrix
$\tC_\theta\inv$ for the parameters $\vtheta$ is just a
projection of the $N\times N$ precision matrix $\tC_v\inv$ for the
data, projected with the derivatives
\begin{equation}
  \tC_\theta\inv(\valpha) = \left[\frac{\dd\vv}{\dd\vtheta}\right]_{\valpha}\T\cdot\tC_v\inv\cdot\left[\frac{\dd\vv}{\dd\vtheta}\right]_{\valpha}
\hquad .
\end{equation}
Remember that $\tC_\theta\inv$ is an implicit function of the
nonlinear parameters $\valpha$.

We don't have priors on period $P$ or eccentricity $e$, but we do on
the fiducial time $t_0$ and argument of perihelion $\varpi$ so we can
take an expectation for $\tC_\theta\inv$ over these latter parameters.
\begin{align}
  \mean{\tC_\theta\inv} &= \expectation{\tC_\theta\inv\given P, e}
  \\
                        &= \int \tC_\theta\inv\,p(t_0\given P)\,p(\varpi)\,\dd t_0\,\dd\varpi
\hquad .
\end{align}
This expectation integral gives the mean amount of information we
expect about the linear parameters $\vtheta$ given the observations
and the expected prior pdfs over $t_0, \varpi$.
Since we have only integrated over these latter two parameters, the
mean information $\mean{\tC_\theta\inv}$ is implicitly a function of
the remaining parameters $P, e$.

Now how do we use this $2\times 2$ mean precision matrix
$\mean{\tC_\theta\inv}$?  There are many things to say here, but the
expected squared uncertainty $\sigma^2_\kappa$ in the velocity
amplitude $\kappa$ will be the $1,1$ element of the inverse of this
precision matrix.
That is
\begin{align}
  \sigma^2_\kappa &= \vu_\kappa\T\cdot\mean{\tC_\theta\inv}\inv\cdot\vu_\kappa
  \\
  \vu_\kappa &\equiv \left[\begin{array}{c} 1 \\ 0 \end{array}\right]
  \hquad,
\end{align}
where (for you geometry nerds) $\vu_\kappa$ is the unit vector in the
$\vtheta$ space pointing in the $\kappa$ direction.

\paragraph{Discussion and implementation notes:}
This solution assumed a very large number of things. We will discuss these
below in \sectionname~\ref{sec:discussion}. Most importantly, it effectively
assumed that the noise on the radial-velocity measurements is zero mean,
Gaussian, and uncorrelated. None of these things is true in any real
set of data on any real star. So in this sense, the solution is a bound.
It is literally the Cram\'er--Rao bound\footnote{HOGG CITE}, and you might
do way worse than this bound, but for extremely deep statistical and
mathematical reasons, you can't do better (unless you are wrong about your
measurement uncertainties $\setof{\sigma_n}_{n=1}^N$ or are willing to live
with substantial bias).

The solution involved some linear algebra (to get the $2\times 2$
precision matrix $\tC_\theta\inv$), and a double integral over $t_0,
\varpi$ (to get the expected precision matrix
$\mean{\tC_\theta\inv}$), and then another inversion (to extract the
expected squared uncertainty $\sigma^2_\kappa$). Each of these has
implementation issues or considerations. One is that, since it
is amazingly sparse, you should never explicitly construct $\tC_v\inv$
in any code you write. You should just use equivalent element-wise
weighting.
Another implementation consideration is that you need to do a two-dimensional
integral. This integral is over a uniform distribution, and the information
in the integrand will (except in rare cases) be very smooth, so you don't
have to do this integral on an extremely fine grid or with any cleverness,
really.

In the above, we inverted the (mean) inverse covariance matrix. Huh?
You might think that the \emph{inverse} of the squared uncertainty in $\kappa$ would be
the $1,1$ element of the precision matrix $\mean{\tC_\theta\inv}$.
This element is the inverse squared uncertainty \emph{conditioned} on
a chosen value of the other linear parameter $v_0$.
The $1,1$ element of the inverse of the precision matrix is HOGG SENTENCE FRAGMENT.

And, relatedly, why did we average $\tC_\theta\inv$? Why not $\tC_\theta$?
They are different!

HOGG What happens if you expect covariant noise?

\section{Examples}
randomly observed vs over a decade; information at some P, e.

uniform observing worse?

\section{Discussion}\label{sec:discussion}

We have explained a, b, c. We have shown x, y, and z.

It is odd that the problem statement never refers to the data we got at all.
In this sense, what is computed here is a forecast; it is a statement of the
best you could do if all your assumptions hold. What assumptions are these?

It literally is the best you could do:
This is the Cram\'er--Rao bound or the Fisher information.

Comments on units, dimensions and so on.

Comments on the point that precisions increase linearly with observing time.
Is it relevant to talk about the trace or the determinant of the precision
matrix? They are funny things.

Frequentism vs Bayesianism, maybe?

Finally, note that there are critical moments in a project's history in which
the information is rising way, way faster than observing time to the 1/2 power.
Right?

% Render the references
\clearmargin\clearpage\raggedright
\bibliography{refs}

\end{document}
