% This document is part of the RvSensitivity project.
% Copyright 2020 the authors.

% TODO:

% Notes:

\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
\documentclass[modern]{aastex63}

% Load common packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}

% Hogg's issues need to be addressed.
\renewcommand{\twocolumngrid}{\onecolumngrid} % guess what this does HAHAHA!
\setlength{\parindent}{1.1\baselineskip}
\addtolength{\topmargin}{-0.2in}
\addtolength{\textheight}{0.4in}
\sloppy\sloppypar\raggedbottom\frenchspacing % trust in Hogg

% text macros
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\rv}{\acronym{RV}}

% math macros
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}\!}}
\newcommand{\inv}{^{-1}}
\newcommand{\scalar}[1]{#1}
\renewcommand{\vector}[1]{\boldsymbol{#1}}
\renewcommand{\tensor}[1]{\mathbf{#1}}
\renewcommand{\matrix}[1]{\mathsf{#1}}
\newcommand{\normal}{\mathcal{N}\!\,}
\newcommand{\like}{\mathscr{L}}
\newcommand{\lnlike}{\ln\like}

% symbols
\newcommand{\vv}{\vector{v}}
\newcommand{\tSigma}{\tensor{\Sigma}}

\shorttitle{tools for designing exoplanet surveys}
\shortauthors{bedell and hogg}

\begin{document}

\title{Tools for designing an extreme-precision radial-velocity survey for exoplanets}

\author[0000-0001-9907-7742]{Megan Bedell}
\affiliation{Flatiron Institute, a division of the
             Simons Foundation, 162 Fifth Avenue, New York, NY 10010, USA}

\author[0000-0003-2866-9403]{David~W.~Hogg}
\affiliation{Flatiron Institute, a division of the
             Simons Foundation, 162 Fifth Avenue, New York, NY 10010, USA}
\affiliation{Max-Planck-Institut f\"ur Astronomie,
             K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\affiliation{Center for Cosmology and Particle Physics,
             Department of Physics,
             New York University, 726 Broadway,
             New York, NY 10003, USA}

\begin{abstract}\noindent
% Context
New instruments are being built and new projects are being planned
that will make possible the detection of truly Earth-like exoplanets,
with masses, periods, host stars, and insolation all very like those of
Earth.
% Aims
Here we provide ideas and tools for the experimental design of such
projects, with the idea that they might be designed to maximize planet
yield, but also be usable for measuring exoplanet population
parameters.
% Methods
Our main contribution is to articulate and operationalize the
conditions that must be met for a radial-velocity signal to be adopted
or accepted as an exoplanet discovery.
These conditions are:
\textsl{(1)}~Significance of the detected radial-velocity signal,
\textsl{(2)}~specificity with respect to other orbital-companion explanations, and
\textsl{(3)}~distinguishability from other kinds of time variability.
We suggest particular implementations of these three conditions.
Our implementations are approximations to what is really done in
exoplanet-discovery contexts, but because our suggestions are
algorithmic and reproducible, they can be used in experimental-design
work.
% Results
We find that there is little difference in practice between
computationally inexpensive frequentist discovery procedures and their
more expensive Bayesian counterparts.
We find, generically, that it is useful for the observational time
baselines between radial-velocity observations to be heterogeneous;
clustered observations produce higher yields than either unclustered
or regular-grid observation plans.
% Conclusions
In the end, experimental design ought to depend on each individual
project's capabilities, objectives, and expectations; we can't solve
these problems for others. We are providing tools intended to help
diverse projects make their own choices better.
\end{abstract}

% \keywords{}

\section*{~}\clearpage
\section{Introduction} \label{sec:intro}

...Planets have been found by one h*ck of a lot of different methods!
...Comment here on some history of the development from \acronym{FFT}s
through Bayesian evidences.

...Note that this subject is important for two reasons. One is that
there really are controversies in the literature about detections, and
the language for arguing about these things could be standardized and
the considerations made clear.

...But also we need to make critical experimental design decisions,
which involve substantial financial and observing resources. These
need to be made in a framework in which we can predict exoplanet
yields under different assumptions. That is our specific motivation here.

...For example, you could have a survey that does X or a survey that does Y.
It isn't obvious without some kind of forecast which will yield more planets
of some type Z that you care about.

...So here we are going to forecast. And if
we are going to forecast, we need to make a well-defined, automated method
for deciding that we do or could detect an exoplanet of type Z in a data
set with certain properties.

If we had to boil the whole long literature of exoplanet detection
down to basic principles,
we would see three kinds of arguments or conditions that have to be
met for a signal in an \rv\ data set to be considered strong evidence
for an exoplanet discovery:
\begin{description}
\item[Detected]
The exoplanet \rv\ signal must be significantly detected in the
data. This means, in practice, that the null (no exoplanet signal at
all) must be ruled out at good confidence, or (equivalently) that the
radial-velocity amplitude must be detected with some high fractional
precision (some number of sigma, say).
\item[Characterizable]
The exoplanet period (and perhaps other orbital parameters) must be
determined with some precision and (more importantly) not confused
with substantially longer-period, substantially shorter-period, or
(say) beat frequencies from other companions. That is, the exoplanet
signal should be uniquely determined in the sense of having orbital
parameters that are strongly preferred (statistically) to alternative
exoplanet or binary-companion explanations.
\item[Not variability]
The exoplanet explanation of the \rv\ signal ought to be strongly
preferred (statistically) to any (of some reasonable, computable set
of) non-exoplanet stellar-variability signals, for example stellar
rotation, asteroseismic modes, star spots, or stellar activity.
It also ought to be preferred to data signals generated by
time-dependent properties (like temperature or flexure or tellurics)
in the hardware or observing conditions.
\end{description}
In what follows, we operationalize these and then use our
operationalized conditions to perform some toy experimental-design
studies to show how they might be used in practice.

\section{Setup and generalities}

We have $N$ radial-velocity observations $v_n$ taken at barycentric times
$t_n$.
These measurements could be real data or---in the context of experimental
design---artificial data that might be expected under some theoretical
predictions.
Each observation $v_n$ has an observational uncertainty variance
$\sigma^2_n$ but the times are arbitrarily precisely known.
The observational uncertainty variances $\sigma^2_n$ are, in this case,
just from the individual, independent observation that delivered $v_n$
and will not include variance from stellar variabilities.
In what follows, stellar variabilities will be explicitly modeled.
We will assemble the $N$ \rv\ observations into an $N$-vector (column vector) $\vv$
\begin{equation}
  \vv\T \equiv \begin{bmatrix} v_1 & v_2 & v_3 & \cdots & v_N \end{bmatrix}
  \quad ,
\end{equation}
and the noise variance estimates into an $N\times N$ variance tensor
\begin{equation}
  \tSigma \equiv \begin{bmatrix} \sigma_1^2 & 0 & 0 & \cdots & 0 \\
                                 0 & \sigma_2^2 & 0 & \cdots & 0 \\
                                 0 & 0 & \sigma_3^2 & \cdots & 0 \\
                                 \vdots & \vdots & \vdots & & \vdots \\
                                 0 & 0 & 0 & \cdots & \sigma_N^2 \end{bmatrix}
  \quad .
\end{equation}

The generative model for the data is
\begin{equation}
  \vv = \mbox{companions} + \mbox{variability} + \mbox{noise}
  \quad ,
\end{equation}
where ``$\mbox{companions}$'' stands in for the predicted \rv\ signal
from the orbital companions (exoplanets, binary partner stars, and so
on),
``$\mbox{variability}$'' stands in for the predicted \rv\ signal from
stellar variabilities (star spots, stellar rotation, activity,
asteroseismology, and so on),
and
the ``$\mbox{noise}$'' is the independent noise in the obervation,
which by assumption will be drawn from an $N$-dimensional
multivariate Gaussian with mean zero and variance $\tSigma$.

The orbital-companion prediction is parameterized by a parameter
vector $\omega$ for the planet of interest, and a vector $\Omega$ for
all the \emph{other} planets and companions in the system.
That is, we will not assume that there is only one planet in the
system (or not in general).
But the parameter vector $\Omega$ describing all the other planets in
the system (all those other than our planet of interest) will be
considered a nuisance paramter vector, to be profiled or marginalized
out.
In the simplest case, the orbital-companion prediction would be a sum
of gravitational two-body orbits.
It could be the output of a few-body simulation though instead if the
system under study involves significant orbital interactions.

The stellar-variability prediction will be parameterized by a vector
$\alpha$ of variability parameters.
In the simplest case this might be a stellar ``jitter'' term.
In complex cases, this could be a sunspot model or the amplitudes,
frequencies, and coherence times of a set of asteroseismic modes.
The parameter vector $\alpha$ could also include some kind of
covariance-function parameters between some indicator of stellar
activity, such as an absorption-line depth, and the \rv\ signal.
In most stellar-variability models, the stellar variability prediction
will be stochastic, or a probability density or pdf for the \rv\
signal contribution, while the companion prediction will usually be
deterministic.

Importantly we are going to assume that we can compute, for any setting
of the parameters $(\omega, \Omega, \alpha)$, a probability density
(pdf) for the data vector $\vv$. That is, we have a likelihood function
\begin{equation}
\lnlike(\omega,\Omega,\alpha) \equiv \ln p(\vv\given\omega,\Omega,\alpha)
\quad .
\end{equation}

Say what kinds of operations you can do as a frequentist or a Bayesian.

\section{Operationalizing detection conditions}

Hogg: Use what's in the above section to shorten this section.

\subsection{Detected}

In physics and astronomy contexts, a source or signal is often
considered detected if it is detected at some significance level,
3-sigma or 5-sigma or 10-sigma, say. What does this mean for our case?
It means that the amplitude $\kappa$ of the \rv\ signal is measured
with a measurement uncertainty $\sigma_\kappa$ that is smaller than
some fraction $(1/Q)$ of the amplitude $\kappa$, or $\kappa >
Q\,\sigma_\kappa$, where $Q$ is 3, 5, or 10, say.
This requirement can be phrased as a significance requirement or as a
precision requirement, since there are translations between precision
and significance for simple distributions (for example, if the noise
in the observations is close to Gaussian).
In the frequentist context, this kind of precision or significance
threshold is implemented by obtaining a measurement $\hat{\kappa}$ by
optimizing the log-likelihood, and an uncertainty $\sigma_\kappa$ by
taking the square root of the diagonal element of the matrix inverse
of the second derivative of the negative log likelihood with respect
to parameters (HOGG CITE).

Alternatively---but still frequentist---the detection of the signal
can be phrased as a null-hypotheis test: The planet is detected if a
zero amplitude for the signal is ruled out at some well defined
p-value, where p-values of ???, ???, and ??? correspond to 3, 5, and
10 sigma (naively; CITE SOMETHING).
This is implemented by evaluating the probability of the data under
the assumption that there is no planet present.
This kind of detection is less common in astronomy contexts, so we will
drop it for now.

The Bayesian equivalent of the precision threshold is to show that the
mean or median of the posterior pdf for the amplitude $\kappa$
(marginalizing out other parameters) is more than $Q$ root-variances
of the posterior pdf away from $\kappa=0$.
We recommend the mean or median of the posterior pdf, and not mode,
because in general the posterior pdf is made to be integrated, not
differentiated (HOGG CITE).
The posterior pdf has a contribution from the prior pdf, so the Bayesian
condition ought to be modified if the prior pdf somehow already excludes
low values of $\kappa$; in that case a significant part of the detection
isn't coming from the data, but is instead coming from the prior pdf.
(Of course this can happen legitimately if the prior pdf for the \rv\ data
is the posterior pdf from some prior data that are highly informative.)
Another way to say this is that the Bayesian and frequentist detection
requirements are similar as long as the prior pdf is fairly smooth over
the relevant amplitude scales.
Indeed, there is a well-defined limit in which this similarity becomes
an equality, as in the ??? THEOREM (HOGG FIND AND CITE).

The Bayesian equivalent of the null-hypothesis test is to perform a
Bayesian model comparison between a model with a finite-amplitude
exoplanet signal against a model with zero amplitude.
This kind of model comparison involves a challenging, strongly
prior-dependent integral, which makes it computationally expensive.
In principle, also, a model comparison leading to a decision requires
(in addition to the priors) a utility function, but that is beyond the
scope of this work (and indeed has almost never appeared in the
astronomical literature, but see HOGG CITE).
All that said, expensive Bayesian model comparisons \emph{have} been
used in exoplanet detection contexts (CITE) and they will come up
again in the context of the \textsl{Characterizable} condition below,
so although we don't prefer these, they are not off the table in any
sense.

HOGG's conclusion is that we should operationalize \textsl{Detected}
with the maximum-likelihood amplitude and standard uncertainty, with
the Bayesian equivalent of that, and with a Bayesian FML ratio. That's
three methods, which is ANNOYING, but what can we do, given our
audience?  We can show that for reasonable prior choices, they are all
nearly equivalent, so the sensible user should just be frequentist.

\subsection{Characterizable}

So there is a precisely measured amplitude for your proposed exoplanet,
or you can strongly rule out the null.
That doesn't mean that you can characterize the planet well.
For instance, if the planet period is very long (longer than your
survey baseline, for example) then the signal you find could perhaps
be produced by a much higher-mass object at longer period (CITE SOMETHING?).
For another instance, if your time sampling is almost exactly
periodic, or unfortunately gapped, with low cadence, then there will
be multiple period--eccentricity combinations that can fit the same
data very precisely (see, for example, HOGG CITE, figure XXX).
And then there are more obscure (but very real) cases such as having two
planets in a 2:1 resonance appear like one planet with an eccentricity.

Of course there is \emph{always} an inclination degeneracy with \rv-only
exoplanet discoveries: It is $M\,\sin i$, not the mass $M$ of the planet
that sets the \rv\ amplitude $\kappa$.
In the interest of permitting the success of \rv\ exoplanet research,
we ignore this problem, and define characterizability in terms of
the \rv-relevant orbital parameters velocity amplitude $\kappa$,
period $P$, eccentricity $e$, argument of perihelion $\varpi$, and
some phase variable (such as a perihelion passage time $t_0$).

The simplest frequentist approach to the question of
characterizability is to look at likelihood ratios between the
best-fit or maximum-likelihood point in orbital-parameter space (the
claimed exoplanet detection), and other points that are ``far away''
in orbital-parameter space.
In this context, the planet is characterizable if all sufficiently
different settings of the parameters are lower in log-likelihood by
more than some threshold $\Delta L$.
This approach requires the setting of the log-likelihood difference
threshold and some criterion for the orbital parameters being
sufficiently different.

HOGG The Bayesian approach is to integrate the posterior pdf locally...

HOGG In the case of qualitatively different solutions (like going from one
planet to two), the frequentist approach stays the same:
Log-likelihood difference. The Bayesian approach is to perform a
Bayesian model comparison...

HOGG: In the case that a (say) one-planet model is being compared to a
(say) two-planet model, the Bayesian approach becomes very expensive
and involves more questionable assumptions. Under what conditions will
it be substantially better?

\subsection{Not variability}

First business: What kinds of stellar variabilities are tractable, or
convertable into a hypothesis test or parameter estimation? This list
will change as more things are discovered and worked out in the
literature.

Again, frequentist and Bayesian approaches...

\section{Example experimental design experiments}

You have five years; you can observe $M$ stars, each $N$ times. Should
you increase $N$ and reduce $M$ or \foreign{vice versa}? There will
clearly be a sweet spot in $(N,M)$ space: If you only observe one star
($M=1$), you can (at most) discover one planetary system. If you
observe many stars, but you only observe each star once ($N=1$), you
can't find any planet around any of them. How do we find the sweet
spot, and will it be close to our intuitive guesses?

...Note: [WHERE TO PUT THIS?]
These experiments will be one-sided in the following sense: We will
not do exhaustive searches of every possible alternative
orbital-architecture or stellar-variability false positive. That is, we
won't do the full due dilligence in these toy experiments that one might
want to do in a real exoplanet survey. However, the results will be one-sided
in the following sense: If an exoplanet wouldn't pass the tests we are
performing here, it won't perform the more stringent tests that some future
survey might do. So one can think of the experiments we are performing here
as being minimal requirements---necessary but not sufficient---for exoplanet
discovery.

You have decided on $N$ and $M$. Should you observe the $M$ stars as close
to uniformly or periodically as you can? Or should you observe them with
a stochastic---or even deliberately clustered---cadence?

\section{Discussion}

Hello World!

\acknowledgments

It is a pleasure to thank
  Dan Foreman-Mackey (Flatiron),
  Lily Zhao (Yale),
  and
  the \project{Terra Hunting Experiment} team
for help with all the things.

\software{
    % Astropy \citep{astropy, astropy:2018},
    % exoplanet \citep{exoplanet:exoplanet},
    % gala \citep{gala},
    % IPython \citep{ipython},
    % numpy \citep{numpy},
    % pymc3 \citep{Salvatier2016},
    % schwimmbad \citep{schwimmbad:2017},
    % scipy \citep{scipy},
    % theano \citep{theano},
    % thejoker \citep{thejoker, Price-Whelan:2019a}
}

\bibliographystyle{aasjournal}
\bibliography{refs}

\end{document}
